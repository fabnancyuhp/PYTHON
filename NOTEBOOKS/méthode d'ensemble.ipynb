{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging vs boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le but des méthodes d'ensemble est de combiner les prédictions de plusieurs algorithmes. \n",
    "plusieur modele de machine learning pour en creer un plus puissant (plus précis, moins de variance...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les méthodes d'ensembles consiste en une collection de prédiction que l'on rassemble (par exemple moyenne) pour avoir une prédiction finale. <br>\n",
    "Nous verrons ici 2 types de méthode d'ensemble:\n",
    "* les méthodes de bagging (avec la forest aléatoire)\n",
    "* les métodes de boosting (gradient boosting tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les méthodes de bagging : on construit un ensemble d'estimateurs, de predictions, de learners indépendants que l'on combinent (moyenne, mode, medianne...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le méthodes de boosting :  on construit des estimateurs, des prédicteurs, des learners de manière sequentielle. L'estimateurs, le prédicteur, le learner subséquent apprend des erreurs des estimateurs,      prédicteurs,learners précédants."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arbre de décision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un arbre de décision est une classification par série de test successif. Les arbres sont le résultat d'algorithme CART (classification and regression trees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import mglearn\n",
    "mglearn.plots.plot_animal_tree()\n",
    "#display(tree)\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn import tree\n",
    "iris = load_iris()\n",
    "clf = tree.DecisionTreeClassifier(max_depth=3)\n",
    "clf = clf.fit(iris.data, iris.target)\n",
    "import graphviz \n",
    "dot_data = tree.export_graphviz(clf, out_file=None) \n",
    "graph = graphviz.Source(dot_data) \n",
    "graph.render(\"iris\") \n",
    "dot_data = tree.export_graphviz(clf, out_file=None, \n",
    "                         feature_names=iris.feature_names,  \n",
    "                         class_names=iris.target_names,  \n",
    "                         filled=True, rounded=True,  \n",
    "                         special_characters=True)  \n",
    "graph = graphviz.Source(dot_data)  \n",
    "#graph "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn import tree\n",
    "iris = load_iris()\n",
    "clf = tree.DecisionTreeClassifier(max_depth=3)\n",
    "clf = clf.fit(iris.data, iris.target)\n",
    "import graphviz \n",
    "rep = open(\"/home/fabien/Bureau/COURSPYTHON/ensemble/tree.png\",'w')\n",
    "dot_data = tree.export_graphviz(clf, out_file=rep, \n",
    "                         feature_names=iris.feature_names,  \n",
    "                         class_names=iris.target_names,  \n",
    "                         filled=True, rounded=True,  \n",
    "                         special_characters=True) \n",
    "rep.close()\n",
    "#graph = graphviz.Source(dot_data)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dtree.png'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import SVG\n",
    "from graphviz import Source\n",
    "#graph.format = 'png'\n",
    "graph = Source(tree.export_graphviz(clf, out_file=None,feature_names=iris.feature_names,class_names=iris.target_names))\n",
    "#SVG(graph.pipe(format='svg'))\n",
    "graph.format = 'png'\n",
    "graph.render('dtree',view=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ci-dessus, un arbre de décision de profondeur 3 permettant de prédire l'espece des iris."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random forest (forest aléatoire)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On se donne un ensemble d'apprentissage $\\{(x^{1},y^{1}),\\dots,(x^{n},y^{n})\\}$. $x^{i}\\in \\mathbb{R}^{m}$.\n",
    "* Dans les problème de classification :$y^{i} \\in \\{-1,1\\}$. \n",
    "* Dans les problème de régréssion : $y^{i}\\in \\mathbb{R}$\n",
    "\n",
    "L'espace des features est de dimention m, on a m features.<br>\n",
    "Avec la stratégie one versul all ou bien la strategie one vs one les forest aléatoire traite de problème multiclasse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une forest aléatoire est un ensemble d'arbre de décision indépendants. Pour construire la foret aléatoire, \n",
    "* on doit choisir le nombre d'arbre (n_estimator dans les algorithmes RandomForestClassifier et RandomForestRegressor) \n",
    "* le nombre de features maximum que l'on utilise lors de la division d'un neux (max_feature=p,p<=m). Lors du scindage d'un neux, la division qui est choisit est la meilleur parmis le sous ensemble alatoire de p features.\n",
    "* la fonction qui mesure la qualité des divisions des neux de chaque arbre \n",
    "     * criterion ='gini', 'entropy' pour la classification\n",
    "     *           ='mse', 'mae' pour la regression\n",
    "* La profondeur maximal de chaque arbre (max_depth)\n",
    "* Les autres citéres des arbres de décision\n",
    "\n",
    "Chaque arbre est construit avec l'algorithme CART"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour faire une prédiction avec un algorithme de la foret aléatoire\n",
    "* On fait une prediction pour chaque arbre de la foret\n",
    "* Ensuite :\n",
    "  * Pour la régréssion, la moyenne des résultats précédants produit la prédiction finale\n",
    "  * Dans le cas d'une classification, la prédiction finale est le mode des résultats précédant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithme de la forest aléatoire\n",
    "Entrées:\n",
    "* $x$, l'observation à prévoir\n",
    "* $\\{(x^{1},y^{1}),\\dots,(x^{n},y^{n})\\}$ l'échantillon d'apprentissage\n",
    "* B le nombre d'arbre (n_estimator en python)\n",
    "* $p \\in \\mathbb{N}^{*}$ le nombre le nombre de features candidats pour découper un noeud (max_feature en python)\n",
    "\n",
    "Pour m=1 à B:\n",
    "* tirer un échantillon bootstrap de $\\{(x^{1},y^{1}),\\dots,(x^{n},y^{n})\\}$\n",
    "* Construire un arbre CART sur cet échantillon bootstrap, chaque coupure est sélectionnée en minimisant la fonction de coût de CART sur un ensemble de p features choisis au hasard parmi les m. On note $h_{m}$ l'arbre construit.\n",
    "\n",
    "Sortie\n",
    "* l'estimateur $F(x)=\\frac{1}{B}\\sum_{m=1}^{B}h_{m}(x)$ pour une régression\n",
    "* l'estimateur $F(X)=mode(h_{1}(X),\\dots,h_{m}(X)$ pour une classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fonctionnement du Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient boosting implique 3 éléments:\n",
    "* une fonction de perte à optimiser.\n",
    "* un \"weak learner\" pour faire des prédiction.\n",
    "* un modéle additife additionnant les weak learners dans le but de minimiser la fonction de perte\n",
    "\n",
    "L'algorithme du gradient boosting tree permet de traiter de problème de classification binaire, classification multilcasse et de régression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modele additife"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'algorithme du gradient boosting tree produit une fonction de la forme:\n",
    "$$F(x)=\\sum_{m=1}^{M}\\gamma_{m}h_{m}(x)$$\n",
    "ou les $h_{m}$ sont des arbre de décision de taille fixe. Ces arbre sont qualifé de \"weak learners\" tandis que $F(x)$ est qualifié de strong learner. On parle de modele aditif car $F(x)$ est une somme."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fonction de perte \n",
    "La fonction de perte utilisée depends du type de problème (classification  ou régression) que l'on doit résoudre. Les fonctions de perte doivent être différentiable. \n",
    "En sklearn, on utilisera les fonctions de pertes suivantes:\n",
    "* logarithmic loss:  classifcation à 2 classes $L(Y,f(x))=\\ln\\left(1+e^{-2Yf(x)}\\right)$\n",
    "* Exponential loss (classification à 2 classes $L(Y,f(x))=e^{-yf(x)}$\n",
    "* squared error (regression) $L(Y,f(x))=(Y-f(x))^{2}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L'algorithme du gradient \n",
    "On se donne un échantillon $\\{(x^{1},y^{1}),\\dots,(x^{n},y^{n})\\}$ $x^{i}\\in \\mathbb{R}^{m}$.\n",
    "* Dans les problème de classification :$y^{i} \\in \\{-1,1\\}$. \n",
    "* Dans les problème de régréssion : $y^{i}\\in \\mathbb{R}$\n",
    "\n",
    "L'algorithme produit une fonction:\n",
    "$$F(x)=\\sum_{i=1}^{M}\\gamma_{m}h_{m}(x)$$\n",
    "ou les $h_{m}$ sont des arbres (décision et regression) de taille fixe. F est construit à l'aide de M itérations (M arbres):\n",
    "$$F_{m}(x)=F_{m-1}(x)+v\\gamma_{m}h_{m}(x) $$\n",
    "ou le nouvel arbre $h_{m}$ est solution du problème:\n",
    "$$h_{m}=\\underset{h}{argmin}\\sum_{i=1}^{n}L(y^{i},F_{m-1}(x^{i})+h(x^{i})) $$\n",
    "Le modèle initial $F_{0}$ est specifique au problème, dans le cas d'un problème de régression on utilise la moyenne des valeurs cibles. L désigne une fonction de perte. $v$ est le paramètre du taux d'apprentissage (learning rate).\n",
    "On a:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a:\n",
    "$$F_{m}(x)=F_{m-1}(x)-\\gamma_{m}\\sum_{i=1}^{n}\\nabla_{F}(y^{i},F_{m-1}(x^{i})) $$\n",
    "ou\n",
    "$$\\gamma_{m}=\\underset{\\gamma}{argmin}\\sum_{i=1}^{n}L(y^{i},F_{m-1}(x^{i})-\\gamma \\frac{\\partial L(y^{i},F_{m-1}(x^{i}))}{\\partial F_{m-1}(x^{i})})$$\n",
    "Les algorithmes pour la  regression et la classification differ dans la fonction de perte L que l'on utilise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Les paramètres du gradient boosting tree\n",
    "* Le nombre d'arbre impliqué se controle par le paramêtre n_estimators\n",
    "* La profondeur de chaque est controlé par le paramêtre max_depth\n",
    "* Le taux d'apprentissage corespond au paramêtre learning_rate\n",
    "* max_features : le nombre de features que l'on utilise pour le meilleur split "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exemples classification\n",
    "Nous allons utiliser les donnée digit. la variable à predire à 9 classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importation des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "digit = load_digits()\n",
    "X = digit['data']\n",
    "Y = digit['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Division en échantillon d'apprentissage et de test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.25\\\n",
    "                                                 ,random_state=1998)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification de donnée digit avec une forest aléatoire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf1 = RandomForestClassifier(n_estimators=60,max_depth=6,\\\n",
    "                             max_features=0.6,random_state=1998)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On entraine le forest aléatoire clf1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=6, max_features=0.6, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=60, n_jobs=1,\n",
       "            oob_score=False, random_state=1998, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf1.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On fait la prédiction sur X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_pred = clf1.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On calcul la precision du classiffieur clf1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.94"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(Y_test,Y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification de donnée digit avec gradient boosting tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "clf2 = GradientBoostingClassifier(loss='deviance',learning_rate=0.1,\\\n",
    "                                  n_estimators=100,\\\n",
    "                                  max_depth=4,random_state=1998)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On entraine le gradient boosting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=0.1, loss='deviance', max_depth=4,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "              presort='auto', random_state=1998, subsample=1.0, verbose=0,\n",
       "              warm_start=False)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf2.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On fait une prédiction sur X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_pred = clf2.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On calcule la précision du gradient boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9577777777777777"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(Y_test,Y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exemple régression\n",
    "Nous allons prédire la température critique des donnée superconductivity. Les données proviennent de https://archive.ics.uci.edu/ml/datasets/Superconductivty+Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importation des données, dummiefication des variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "rep = \"C:/Users/IFDU1270/Desktop/DATABASE/superconduct/train.csv\"\n",
    "SUPERCONDUCT = pd.read_csv(rep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On creer Y correspondant à la valeur cible et X correspondant au features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "Y = SUPERCONDUCT['critical_temp']\n",
    "X = SUPERCONDUCT.drop(columns=['critical_temp'],axis=1)\n",
    "\n",
    "X_ADUM = pd.get_dummies(X['number_of_elements'].astype(str),prefix='elements')\n",
    "X = X.drop(columns=['number_of_elements'],axis=1)\n",
    "X = pd.concat([X_ADUM,X],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On divise en échantillon d'apprentissage et de test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.3,\\\n",
    "                                                random_state=1998)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RandomForestRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On créer un modele de RandomForestRegressor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "reg1 = RandomForestRegressor(n_estimators=110,max_depth=25,\\\n",
    "                             random_state=1998,max_features='auto')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On lance la phase d'apprentissage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=25,\n",
       "           max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=110, n_jobs=1,\n",
       "           oob_score=False, random_state=1998, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg1.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On fait une prédiction sur X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_pred = reg1.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On calcule le score R2 de la prédiction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9218914641594653"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "r2_score(Y_test,Y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "reg2 = GradientBoostingRegressor(learning_rate=0.05, n_estimators=200,\\\n",
    "                                max_depth=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On lance l'étape d'aprentissage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
       "             learning_rate=0.05, loss='ls', max_depth=5, max_features=None,\n",
       "             max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "             min_impurity_split=None, min_samples_leaf=1,\n",
       "             min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "             n_estimators=200, presort='auto', random_state=None,\n",
       "             subsample=1.0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg2.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On predit sur les données X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_pred = reg2.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On calcule le score r2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9105603420543338"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "r2_score(Y_test,Y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Référence \n",
    "Voici quelques référence:\n",
    "https://machinelearningmastery.com/gentle-introduction-gradient-boosting-algorithm-machine-learning/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
